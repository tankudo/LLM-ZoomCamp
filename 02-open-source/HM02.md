## Question 1. Running Ollama with Docker. What's the version?
```bash
docker run -it
  --rm \
  -v ollama:/root.ollama \
  -p 11435:11434 \
  --name ollama \
  ollama\ollama 
```
```bash
docker exec -it ollama /bin/bash
```
```terminal
ollama --version
```

## Question 2. Downloading an LLM. Manifest file
```terminal
ollama pull gemma:2b

cd /root/.ollama

cd models/manifests/registry.ollama.ai/library
```

## Question 3. Running the LLM. Output from 10 * 10 
```terminal
ollama run gemma:2b
```

## Question 4. Downloading the weights. Size of the folder
```bash
mkdir ollama_files

docker run -it \
    --rm \
    -v ./ollama_files:/root/.ollama \
    -p 11434:11434 \
    --name ollama \
    ollama/ollama
```
```bash
docker exec -it ollama ollama pull gemma:2b
```
```bash
du -h
```

## Question 5. Adding the weights. Dockerfile
```bash
FROM ollama/ollama

COPY ./ollama_files /root/.ollama

```
## Question 6. Serving the LLM. Number of output tokens
```bash
docker build -t ollama-gemma2b .

docker run -it --rm -p 11434:11434 ollama-gemma2b
```
